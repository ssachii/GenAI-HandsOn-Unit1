# GenAI-HandsOn-Unit1
Gen AI | Sem VI | Unit 1


## Unit 1: Generative AI & NLP

### Overview
This repository contains all work completed for **Unit 1: Generative AI & NLP**, covering theory, hands-on experiments, model benchmarking, and a final mini-project (Recipe Generator).

### Phase 1: Theory & Concepts
- Studied core concepts of **AI, ML, DL**, and **LLMs**
- Understood **Transformer architecture**
- Learned NLP fundamentals: tokenization, NER, and text generation
- Used the Unit 1 mind map to connect concepts

### Phase 2: Hands-On Practice
- Implemented Hugging Face **pipelines** in Python
- Compared **fast vs smart models** (`distilgpt2` vs `gpt2`)
- Visualized **tokenization** and **Named Entity Recognition**
- Ran **summarization, Q&A, and masked language modeling**

### Phase 3: Assessment (Benchmarking)
- Benchmarked **BERT, RoBERTa, and BART** across:
  - Text Generation
  - Fill-Mask (MLM)
  - Question Answering
- Observed model success/failure based on architecture
- Analyzed outputs and explained unexpected behavior

### Phase 4: Project – Recipe Generator
- Built a Generative AI–based **Recipe Generator**
- Takes ingredients as input and generates structured recipes
- Demonstrates practical use of **text generation models**

### Notebooks
- `Unit1_HandsOn.ipynb`
- `Unit1_Benchmark.ipynb`
- `Unit1_RecipeGenerator.ipynb`

## Key Insight
There is no “one-model-fits-all.” Model architecture determines what a model can and cannot do.
