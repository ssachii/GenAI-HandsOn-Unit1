{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6c0c54e-d66e-42cd-8042-ada6ae3dfa55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import os\n",
    "import nltk\n",
    "from transformers import set_seed, pipeline, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1df09776-290f-47f8-8b7a-bcfd176ed2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"unit 1.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd6f6b8a-22c7-4792-ba79-e87530c6c434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    print(\"File loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{file_path}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1407c89c-7ed3-4c39-96c5-c7be8d8eba6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Preview ---\n",
      "Generative AI and Its Applications: A Foundational Briefing\n",
      "\n",
      "Executive Summary\n",
      "\n",
      "This document provides a comprehensive overview of Generative AI, synthesizing foundational concepts, technological underpinnings, and practical applications as outlined in the course materials from PES University. Generative AI represents a transformative subset of Artificial Intelligence focused on creating novel content, a capability primarily driven by the advent of Large Language Models (LLMs). The evolution of ...\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Data Preview ---\")\n",
    "print(text[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67719313-fadd-4de3-b893-fb7583f166ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2e9ac86-c799-4d48-a320-c41aaf39624c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Generative AI is a revolutionary technology that\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c178363-fac2-4292-9db7-44b0d70d5b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI is a revolutionary technology that is designed to work with existing algorithms and will revolutionize science to the next stage and contribute to future research. Today's open source AI makes artificial intelligence an imperative, and our focus is on advancing AI at its\n"
     ]
    }
   ],
   "source": [
    "# Initialize the pipeline with the specific model\n",
    "fast_generator = pipeline('text-generation', model='distilgpt2')\n",
    "\n",
    "# Generate text\n",
    "output_fast = fast_generator(prompt, max_length=50, num_return_sequences=1)\n",
    "print(output_fast[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35bfa8a2-fd6c-4d45-9053-d87db4d696be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI is a revolutionary technology that could revolutionize the way we think about and apply artificial intelligence. It can detect patterns (including emotions) and produce artificial intelligence that is more intuitive. But it can't predict what a user might be doing,\n"
     ]
    }
   ],
   "source": [
    "smart_generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "output_smart = smart_generator(prompt, max_length=50, num_return_sequences=1)\n",
    "print(output_smart[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "720c7426-83a1-4a5a-aeea-a7fd632c4bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "110fe6ab-3302-4d1b-a29b-fd7de39a9532",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence = \"Transformers revolutionized NLP.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1fc3589-6001-48f3-abaf-69727ce70f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Transform', 'ers', 'Ä revolution', 'ized', 'Ä N', 'LP', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sample_sentence)\n",
    "print(f\"Tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9028a0af-4ef8-43f0-a2af-284bfe273f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [41762, 364, 5854, 1143, 399, 19930, 13]\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"Token IDs: {token_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa4a1032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/apple/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/apple/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "import nltk\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9465595-cd99-4734-b05b-73d18da8cfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('Transformers', 'NNS'), ('revolutionized', 'VBD'), ('NLP', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "pos_tags = nltk.pos_tag(nltk.word_tokenize(sample_sentence))\n",
    "print(f\"POS Tags: {pos_tags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fc67f83-26d5-4b45-a2a0-980dcdd75548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "601b8779-a73c-4926-b6cd-2cd2f1f0b5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity               | Type       | Score\n",
      "---------------------------------------------\n",
      "AI                   | MISC       | 0.98\n",
      "PES University       | ORG        | 0.99\n",
      "AI                   | MISC       | 0.98\n",
      "Large Language Models | MISC       | 0.91\n",
      "LLMs                 | MISC       | 0.90\n",
      "Transformer          | MISC       | 0.99\n"
     ]
    }
   ],
   "source": [
    "snippet = text[:1000]\n",
    "entities = ner_pipeline(snippet)\n",
    "\n",
    "print(f\"{'Entity':<20} | {'Type':<10} | {'Score':<5}\")\n",
    "print(\"-\"*45)\n",
    "for entity in entities:\n",
    "    if entity['score'] > 0.90:\n",
    "        print(f\"{entity['word']:<20} | {entity['entity_group']:<10} | {entity['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "330c9cbc-a6c3-446e-98f8-13a6ba8bcf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_section = \"\"\"\n",
    "The introduction of the Transformer architecture in the 2017 paper \"Attention is all you need\" was a watershed moment in AI. It provided a more effective and scalable way to handle sequential data like text, replacing older, less efficient methods like recurrence (RNNs) and convolutions.\n",
    "The fundamental innovation of the Transformer is the attention mechanism. This component allows the model to weigh the importance of different words (tokens) in the input sequence when making a prediction. In essence, for each word it processes, the model can \"pay attention\" to all other words in the input, helping it understand context, resolve ambiguity, and handle long-range dependencies. This is crucial for tasks like translation, summarization, and question answering.\n",
    "The Transformer architecture consists of an encoder stack (to process the input) and a decoder stack (to generate the output), both of which heavily utilize multi-head attention and feed-forward networks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0895513d-49a6-4619-81eb-d2d855c98f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The introduction of the Transformer architecture in the 2017 paper \"Attention is all you need\" was a watershed moment in AI . It provided a more effective and scalable way to handle sequential data like text, replacing older, less efficient methods like recurrence (RNNs) and conv\n"
     ]
    }
   ],
   "source": [
    "fast_sum = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "res_fast = fast_sum(transformer_section, max_length=60, min_length=30, do_sample=False)\n",
    "print(res_fast[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca7ff6a7-e41e-4aa9-a88d-45843c337538",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The introduction of the Transformer architecture in the 2017 paper \"Attention is all you need\" was a watershed moment in AI. It provided a more effective and scalable way to handle sequential data like text.\n"
     ]
    }
   ],
   "source": [
    "smart_sum = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "res_smart = smart_sum(transformer_section, max_length=60, min_length=30, do_sample=False)\n",
    "print(res_smart[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c344068d-5995-4308-9bae-b98cd0176452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40a3555d-1cdf-4034-9dbb-1d87681ebf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What is the fundamental innovation of the Transformer?\n",
      "A: to identify hidden patterns, structures, and relationships within the data\n",
      "\n",
      "Q: What are the risks of using Generative AI?\n",
      "A: data privacy, intellectual property, and academic integrity\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is the fundamental innovation of the Transformer?\",\n",
    "    \"What are the risks of using Generative AI?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    res = qa_pipeline(question=q, context=text[:5000])\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {res['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04584cea-395c-4bb2-8805-a5305e3802fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "mask_filler = pipeline(\"fill-mask\", model=\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87af5dd5-1a3b-4091-a06c-8aa4b37d6cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applications: 0.06\n",
      "ideas: 0.05\n",
      "problems: 0.05\n",
      "systems: 0.04\n",
      "information: 0.03\n"
     ]
    }
   ],
   "source": [
    "masked_sentence = \"The goal of Generative AI is to create new [MASK].\"\n",
    "preds = mask_filler(masked_sentence)\n",
    "\n",
    "for p in preds:\n",
    "    print(f\"{p['token_str']}: {p['score']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
