{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2704d6ec-eaf6-44bb-996f-34e9487ca2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.46.1)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.7.1)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.25.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: torchaudio in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.7.1)\n",
      "Requirement already satisfied: tensorflow in /Users/apple/Library/Python/3.12/lib/python/site-packages (2.20.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (2025.5.1)\n",
      "Collecting torch\n",
      "  Downloading torch-2.10.0-cp312-none-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torchvision) (11.2.1)\n",
      "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.10.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/apple/Library/Python/3.12/lib/python/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/apple/Library/Python/3.12/lib/python/site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/apple/Library/Python/3.12/lib/python/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in /Users/apple/Library/Python/3.12/lib/python/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/apple/Library/Python/3.12/lib/python/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in /Users/apple/Library/Python/3.12/lib/python/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in /Users/apple/Library/Python/3.12/lib/python/site-packages (from tensorflow) (6.32.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/apple/Library/Python/3.12/lib/python/site-packages (from tensorflow) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.73.1)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in /Users/apple/Library/Python/3.12/lib/python/site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in /Users/apple/Library/Python/3.12/lib/python/site-packages (from tensorflow) (3.11.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Users/apple/Library/Python/3.12/lib/python/site-packages (from tensorflow) (3.15.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /Users/apple/Library/Python/3.12/lib/python/site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/apple/Library/Python/3.12/lib/python/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (1.1.5)\n",
      "Requirement already satisfied: rich in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.10.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in /Users/apple/Library/Python/3.12/lib/python/site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /Users/apple/Library/Python/3.12/lib/python/site-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/apple/Library/Python/3.12/lib/python/site-packages (from requests->transformers) (2026.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->keras>=3.10.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "Downloading torchvision-0.25.0-cp312-cp312-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.10.0-cp312-none-macosx_11_0_arm64.whl (79.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.10.0-cp312-cp312-macosx_11_0_arm64.whl (737 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m737.1/737.1 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch, torchvision, torchaudio\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "descript-audiotools-unofficial 0.7.4 requires protobuf!=4.24.0,<5.0.0,>=3.19.6, but you have protobuf 6.32.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.10.0 torchaudio-2.10.0 torchvision-0.25.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.12 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers torch torchvision torchaudio tensorflow\n",
    "from transformers import pipeline, set_seed, GPT2Tokenizer\n",
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578a8349-97ba-4d2a-ab4e-2475cebd7d24",
   "metadata": {},
   "source": [
    "# Experiment 1: Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "210f1c90-fbb8-413b-83e5-8046df6bd1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e9cbaea-d21c-4300-9338-9c5c8d8e926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The future of Artificial Intelligence is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3852b8-c51b-409b-99a2-d15661d699e9",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43f867e8-1c55-42f9-8094-3661618a8ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of Artificial Intelligence is.................\n"
     ]
    }
   ],
   "source": [
    "fast_generator = pipeline('text-generation', model=\"bert-base-uncased\")\n",
    "output_fast = fast_generator(prompt, max_length=25, num_return_sequences=1)\n",
    "print(output_fast[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f33d7f8-7253-42a0-8d86-596d0f2a4ee0",
   "metadata": {},
   "source": [
    "### RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a2aa7f9-59cd-41eb-8931-68dc469f9166",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of Artificial Intelligence is\n"
     ]
    }
   ],
   "source": [
    "fast_generator = pipeline('text-generation', model=\"roberta-base\")\n",
    "output_fast = fast_generator(prompt, max_length=25, num_return_sequences=1)\n",
    "print(output_fast[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447eeafe-65b3-4490-b5e2-4eb1863aa2e1",
   "metadata": {},
   "source": [
    "### BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fa4440e-f2cb-4949-ac62-116c22f83b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of Artificial Intelligence isokes sectionsippedippedbankbankbankokesokes �bankbank autopsy autopsybankbank\n"
     ]
    }
   ],
   "source": [
    "fast_generator = pipeline('text-generation', model=\"facebook/bart-base\")\n",
    "output_fast = fast_generator(prompt, max_length=25, num_return_sequences=1)\n",
    "print(output_fast[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10356483-ff90-428a-9965-12243be54f64",
   "metadata": {},
   "source": [
    "# Experiment 2: Masked Language Modeling (Missing Word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56827c24-1d11-4796-a6a5-cf685c9bdd84",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9095c512-7299-4a46-8842-b14c4f1337e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "mask_filler = pipeline(\"fill-mask\", model=\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1cd94a18-abce-448c-8509-7625ec3553ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create: 0.54\n",
      "generate: 0.16\n",
      "produce: 0.05\n",
      "develop: 0.04\n",
      "add: 0.02\n"
     ]
    }
   ],
   "source": [
    "masked_sentence = \"The goal of Generative AI is to [MASK] new content.\"\n",
    "preds = mask_filler(masked_sentence)\n",
    "\n",
    "for p in preds:\n",
    "    print(f\"{p['token_str']}: {p['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e726ed-e41b-48a3-9f4c-a3c4a2db493d",
   "metadata": {},
   "source": [
    "### RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd516a08-5e8f-4552-ae14-8642d0f5fd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "mask_filler = pipeline(\"fill-mask\", model=\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97b367ba-b1a8-4460-a582-92e13b3ab4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " create: 0.37\n",
      " generate: 0.37\n",
      " discover: 0.08\n",
      " find: 0.02\n",
      " provide: 0.02\n"
     ]
    }
   ],
   "source": [
    "masked_sentence = \"The goal of Generative AI is to <mask> new content.\"\n",
    "preds = mask_filler(masked_sentence)\n",
    "\n",
    "for p in preds:\n",
    "    print(f\"{p['token_str']}: {p['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd70cf8-6de3-4fc2-8e22-35c597bb15a2",
   "metadata": {},
   "source": [
    "### BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ddadac5-982e-4fb2-80d2-0e8391b37a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "mask_filler = pipeline(\"fill-mask\", model=\"facebook/bart-base\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f20ac579-75ab-48c2-8cae-0c95ca85ab08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " create: 0.07\n",
      " help: 0.07\n",
      " provide: 0.06\n",
      " enable: 0.04\n",
      " improve: 0.03\n"
     ]
    }
   ],
   "source": [
    "masked_sentence = \"The goal of Generative AI is to <mask> new content.\"\n",
    "preds = mask_filler(masked_sentence)\n",
    "\n",
    "for p in preds:\n",
    "    print(f\"{p['token_str']}: {p['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a86ac13-3359-4647-8594-7ef2c7bc7ce5",
   "metadata": {},
   "source": [
    "# Experiment 3: Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5773b2d-d76c-4a17-a805-2414f887c772",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the risks?\"\n",
    "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5258a79-74f5-4c9d-91f7-8d1efcf8ab94",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "63d50431-ab8d-4ce9-b416-e7e3c2f72f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "qa_pipeline = pipeline(\"question-answering\", model=\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7687d97f-62e4-4eb2-b6ec-ade0fe471584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What are the risks?\n",
      "A: hallucinations\n"
     ]
    }
   ],
   "source": [
    "res = qa_pipeline(question=question, context=context)\n",
    "print(f\"Q: {question}\")\n",
    "print(f\"A: {res['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abec252-5008-42f1-92cf-baaa7508816e",
   "metadata": {},
   "source": [
    "### RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb7c3a12-f5a9-4e05-b6e8-05810667e7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "qa_pipeline = pipeline(\"question-answering\", model=\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e4fea24-833d-48fa-8c67-ee0e13fea0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What are the risks?\n",
      "A: Generative AI poses\n"
     ]
    }
   ],
   "source": [
    "res = qa_pipeline(question=question, context=context)\n",
    "print(f\"Q: {question}\")\n",
    "print(f\"A: {res['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50b69d2-4c15-4695-b5eb-6e90221e2258",
   "metadata": {},
   "source": [
    "### BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce222d90-043c-40ab-a73a-3d8485a84180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "qa_pipeline = pipeline(\"question-answering\", model=\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37119e95-a845-4517-9728-d38f0ebd3e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What are the risks?\n",
      "A: Generative AI poses significant risks\n"
     ]
    }
   ],
   "source": [
    "res = qa_pipeline(question=question, context=context)\n",
    "print(f\"Q: {question}\")\n",
    "print(f\"A: {res['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba83165-cb63-4b17-a900-f3b76cf54b0d",
   "metadata": {},
   "source": [
    "# Observation Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4284aa9f",
   "metadata": {},
   "source": [
    "| Task               | Model     | Classification (Success / Failure) | Observation (What actually happened?)                                                                 | Why did this happen? (Architectural Reason) |\n",
    "|--------------------|-----------|------------------------------------|----------------------------------------------------------------------------------------------------------|---------------------------------------------|\n",
    "| Text Generation    | BERT      | Failure                            | Generated only dots (`.................`) instead of a meaningful continuation                           | BERT is an encoder-only model trained using Masked Language Modeling (MLM). It does not learn next-token prediction, which is required for autoregressive text generation. |\n",
    "|                    | RoBERTa   | Failure                            | Returned only the original prompt without any continuation                                                | Like BERT, RoBERTa is encoder-only and trained with MLM. It lacks a causal decoder, so it cannot generate new tokens sequentially. |\n",
    "|                    | BART      | Failure                            | Produced incoherent, repetitive, and nonsensical text (`bankbank autopsy autopsy...`)                   | BART is an encoder–decoder model trained for denoising, not causal generation. Using it with a decoder-only text-generation pipeline causes unstable output. |\n",
    "| Fill-Mask (MLM)    | BERT      | Success                            | Correctly predicted relevant verbs with high confidence: `create (0.54)`, `generate (0.16)`             | BERT is explicitly trained for masked token prediction, making it well-suited for MLM tasks. |\n",
    "|                    | RoBERTa   | Success                            | Produced strong and semantically correct predictions: `create (0.37)`, `generate (0.37)`               | RoBERTa improves MLM training with more data, dynamic masking, and removal of NSP, resulting in better contextual understanding. |\n",
    "|                    | BART      | Partial Success                    | Returned reasonable but low-confidence predictions: `create (0.07)`, `help (0.07)`, `provide (0.06)`   | BART uses span corruption and denoising objectives rather than single-token MLM, so it is less precise for fill-mask tasks. |\n",
    "| Question Answering | BERT      | Partial Success                    | Returned only one correct risk: `hallucinations`                                                         | Base BERT is not fine-tuned for extractive QA. Without task-specific fine-tuning, it often predicts incomplete answer spans. |\n",
    "|                    | RoBERTa   | Failure                            | Returned an incorrect span: `Generative AI poses`                                                        | Although RoBERTa has strong representations, it requires QA fine-tuning to accurately predict start–end answer spans. |\n",
    "|                    | BART      | Partial Success                    | Returned an overlong answer: `Generative AI poses significant risks`                                     | BART-base is not optimized for extractive QA. Without fine-tuning, it tends to select broader spans instead of concise answers. |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
